# Linear-Attention-Recurrent-Neural-Network
A fixed-size go-back-`k` recurrent attention module on an RNN so as to have linear short-term memory by the means of attention.
